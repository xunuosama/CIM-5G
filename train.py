import pandas as pd
import numpy as np
import pickle
import joblib
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, learning_curve, cross_val_score
from xgboost import XGBRegressor, callback
from tqdm import tqdm
from sklearn.metrics import r2_score, make_scorer,mean_squared_error, mean_absolute_error
from sklearn.model_selection import ParameterGrid, cross_val_score
import optuna
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error
import xgboost as xgb
from sklearn.inspection import PartialDependenceDisplay
from sklearn.inspection import partial_dependence
from scipy.interpolate import make_interp_spline
plt.rcParams['font.family'] = 'Palatino Linotype'
# ------------------ å‚æ•° ------------------
cv = KFold(n_splits=5, shuffle=True, random_state=123)

# # åŠ è½½æ•°æ®
# file_path = 'table/fishnet_30x30_processed_feature_engineered.csv'
# df = pd.read_csv(file_path)
# X = df.iloc[1:, 0:-1]
# y = df.iloc[1:, -1].values
#
# Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.2, random_state=108)
#
# # è®­ç»ƒæ¨¡å‹
# model = XGBRegressor(
#     learning_rate=0.01194543620793604,
#     max_depth=11,
#     max_leaves=256,
#     grow_policy='depthwise',
#     colsample_bytree=0.9784466574215168,
#     colsample_bylevel=0.6192663461731014,
#     colsample_bynode=0.8189964711585255,
#     subsample=0.6489006448874424,
#     min_child_weight=3,
#     gamma=0.28620923204365845,
#     reg_alpha=1.0476187038195663,
#     reg_lambda=0.0004402425307719034,
#     objective='reg:squarederror',
#     tree_method='hist',
#     seed=667,
#     n_estimators=1000
# )
#
# model.fit(
#     Xtrain, Ytrain,
#     eval_set=[(Xtest, Ytest)],
#     verbose=True
# )
#
# # ä¿å­˜æ¨¡å‹ä¸ºpkl
# model_path = r"E:\Desktop\5Gsimulation_XGboost\pythonProject\xgb_optuna_model_processed_feature_engineered.pkl"
# joblib.dump(model, model_path)
# print(f"Model saved to {model_path}")





# ------------------ optunaè°ƒå‚ ------------------
class TQDMCallback:
    def __init__(self, total):
        self.pbar = tqdm(total=total)

    def __call__(self, study, trial):
        self.pbar.update(1)
        self.pbar.set_postfix({"Best RMSE": f"{study.best_value:.4f}"})

    def close(self):
        self.pbar.close()

def xgb_optuna_tuner_save_model(Xtrain, Ytrain, model_path, n_trials=100, seed=667):
    """
    ä½¿ç”¨ Optuna ä¼˜åŒ– XGBoost (åŸç”ŸAPI) å›å½’æ¨¡å‹å‚æ•°ï¼Œæ”¯æŒæ—©åœä¸è¿›åº¦æ¡ï¼Œä¿å­˜æ¨¡å‹ã€‚

    å‚æ•°ï¼š
        Xtrain, Ytrain: è®­ç»ƒæ•°æ® (numpyæ•°ç»„æˆ–DataFrame)
        model_path: æ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œå¦‚ 'model.pkl'
        n_trials: è°ƒå‚æ¬¡æ•°
        seed: éšæœºç§å­

    è¿”å›ï¼š
        best_model: æœ€ä½³xgboost.Boosteræ¨¡å‹
        best_params: æœ€ä½³å‚æ•°å­—å…¸
        best_score: æœ€ä½³éªŒè¯é›†RMSE
    """

    def objective(trial):
        params = {
            'eta': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),
            'max_depth': trial.suggest_int('max_depth', 1, 12),
            'max_leaves': trial.suggest_categorical('max_leaves', [31, 64, 128, 256]),
            'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 1.0),
            'colsample_bynode': trial.suggest_float('colsample_bynode', 0.6, 1.0),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),
            'gamma': trial.suggest_float('gamma', 0, 5),
            'reg_alpha': trial.suggest_float('reg_alpha', 0, 2),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 10, log=True),
            'objective': 'reg:squarederror',
            'eval_metric': 'rmse',
            'tree_method': 'hist',
            'seed': seed,
        }

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (20%)
        X_tr, X_val, y_tr, y_val = train_test_split(Xtrain, Ytrain, test_size=0.2, random_state=seed)

        dtrain = xgb.DMatrix(X_tr, label=y_tr)
        dvalid = xgb.DMatrix(X_val, label=y_val)
        evals = [(dtrain, 'train'), (dvalid, 'eval')]

        model = xgb.train(
            params,
            dtrain,
            num_boost_round=1000,
            evals=evals,
            early_stopping_rounds=30,
            verbose_eval=False
        )

        return model.best_score  # éªŒè¯é›†æœ€ä¼˜rmse

    print("ğŸ“¦ å¼€å§‹ Optuna è°ƒå‚ï¼ˆå¸¦æ—©åœå’Œè¿›åº¦æ¡ï¼‰...")

    progress_bar = TQDMCallback(total=n_trials)
    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=seed))
    try:
        study.optimize(objective, n_trials=n_trials, callbacks=[progress_bar])
    finally:
        progress_bar.close()

    best_params = study.best_trial.params
    best_params.update({
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'tree_method': 'hist',
        'seed': seed,
    })

    # ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒå®Œæ•´æ¨¡å‹ï¼ˆæ— éªŒè¯é›†ï¼‰
    dtrain_full = xgb.DMatrix(Xtrain, label=Ytrain)
    best_model = xgb.train(
        best_params,
        dtrain_full,
        num_boost_round=1000,
        verbose_eval=False
    )

    # ä¿å­˜æ¨¡å‹
    best_model.save_model(model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

    return best_model, best_params, study.best_value
# ------------------ shapåˆ†æ ------------------


import pandas as pd
import numpy as np
import joblib
import shap
import matplotlib.pyplot as plt

def shap_selected_features(
    model_path,
    csv_path,
    selected_features=None,  # é»˜è®¤Noneè¡¨ç¤ºç”¨å…¨éƒ¨ç‰¹å¾
    model_format='pkl'
):
    # åŠ è½½æ•°æ®
    df = pd.read_csv(csv_path)
    X_full = df.iloc[1:, 0:-1].copy()  # è·³è¿‡ç¬¬ä¸€è¡Œå’Œæœ€åä¸€åˆ—

    # å¦‚æœæ²¡æœ‰æŒ‡å®šselected_featuresï¼Œå°±ç”¨æ‰€æœ‰ç‰¹å¾
    if selected_features is None:
        X_selected = X_full
    else:
        X_selected = X_full[selected_features]

    # åŠ è½½æ¨¡å‹
    if model_format == 'pkl':
        model = joblib.load(model_path)
    else:
        import xgboost as xgb
        model = xgb.Booster()
        model.load_model(model_path)

    # è®¡ç®— SHAP å€¼
    print("ğŸ” æ­£åœ¨è®¡ç®— SHAP å€¼...")
    explainer = shap.TreeExplainer(model)
    shap_values = explainer(X_selected)

    # ç»˜åˆ¶ SHAP summary plot
    plt.figure()
    shap.summary_plot(shap_values.values, X_selected, show=True)




# ------------------ åˆ†æå„ä¸ªç‰¹å¾å¯¹ç»“æœçš„å½±å“ ------------------

def plot_pdp_selected_features(
    model_path,
    csv_path,
    selected_features=[
        'DEM_center',
        'Power_to_Dist_ratio',
        'True_3D_Dist',
        'ALT_M_',
        'Match_Dist',
        'DEM_center',
        'log_True_3D_Dist',
        'Building_Coverage',
        'SPEED_M_s_',
        'Match_Angle_cos',
        'NDVI_center',
        'Weighted_height',
        'Match_Angle_sin',
        'Match_Dist_bin'
    ],
    model_format='pkl',
    grid_resolution=1000,
    figsize=(6, 4),
    smooth=True
):
    """
    ç¨³å®šã€å…¼å®¹æ‰€æœ‰ sklearn ç‰ˆæœ¬çš„ PDP ç»˜å›¾å‡½æ•°ï¼ˆä»…æå– lines_ æ•°æ®ï¼‰
    """
    # åŠ è½½æ¨¡å‹
    if model_format == 'pkl':
        model = joblib.load(model_path)
    else:
        raise ValueError("ä»…æ”¯æŒ pkl æ ¼å¼æ¨¡å‹")

    # è¯»å–æ•°æ®
    df = pd.read_csv(csv_path)
    X = df.iloc[1:, 0:-1].copy()

    # ä¿ç•™å®é™…å­˜åœ¨çš„ç‰¹å¾
    available_features = [f for f in selected_features if f in X.columns]

    print(f"\nğŸ“Š ç»˜åˆ¶ PDP å›¾ï¼ˆå…± {len(available_features)} ä¸ªç‰¹å¾ï¼‰:")
    for i, feat in enumerate(available_features, 1):
        print(f"{i}. {feat}")

        # ä¸´æ—¶å›¾ç”¨äºç”Ÿæˆ lines_ æ•°æ®ï¼ˆæˆ‘ä»¬ä¸ç›´æ¥æ˜¾ç¤ºå®ƒï¼‰
        fig, ax = plt.subplots(figsize=figsize)
        disp = PartialDependenceDisplay.from_estimator(
            model,
            X,
            features=[feat],
            kind="average",
            grid_resolution=grid_resolution,
            ax=ax
        )

        # æå– lines_ ä¸­çš„æ•°æ®
        x_vals, y_vals = disp.lines_[0][0].get_data()

        plt.close(fig)  # å…³é—­ä¸­é—´å›¾ï¼Œé¿å…å›¾åƒå åŠ æˆ–çª—å£è¿‡å¤š

        # æ‰‹åŠ¨ç»˜å›¾
        plt.figure(figsize=figsize, dpi=300)
        if smooth and len(x_vals) > 10:
            xnew = np.linspace(np.min(x_vals), np.max(x_vals), 300)
            ynew = make_interp_spline(x_vals, y_vals, k=3)(xnew)
            plt.plot(xnew, ynew, color='blue')
        else:
            plt.plot(x_vals, y_vals, color='blue')

        plt.title(f"PDP of {feat}")
        plt.xlabel(feat)
        plt.ylabel("RSRP Prediction")
        plt.grid(True)
        plt.tight_layout()
        # ä¿å­˜å›¾ç‰‡ï¼Œè·¯å¾„å’Œæ–‡ä»¶åå¯æ ¹æ®éœ€è¦ä¿®æ”¹
        save_path = fr"E:\Desktop\Supplementary picture\PDP_{feat}.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()




# ------------------ è°ƒå‚å…¥å£ ------------------
# best_model = grid_search_xgboost(param_grid, Xtrain, Ytrain, cv)
# print("æµ‹è¯•é›†è¯„ä¼° RMSE:", np.sqrt(mean_squared_error(Ytest, best_model.predict(Xtest))))
# best_n_estimators = tune_n_estimators(Xtrain, Ytrain, cv)
# print("å»ºè®®ä½¿ç”¨ n_estimators =", best_n_estimators)

#best_params, results_df = tune_maxdepth_minchildweight(Xtrain, Ytrain, cv)

#best_params, results_df = tune_subsample_colsample(Xtrain, Ytrain, cv=cv)

# #optunaè°ƒå‚
# model_path = r"E:\Desktop\5Gsimulation_XGboost\pythonProject\xgb_optuna_model.json"
# best_model, best_params, best_rmse = xgb_optuna_tuner_save_model(Xtrain, Ytrain, model_path=model_path, n_trials=200)
# print("è°ƒå‚å®Œæˆï¼Œæœ€ä½³å‚æ•°ï¼š", best_params)
# print(f"æœ€ä½³éªŒè¯é›†RMSE: {best_rmse:.4f}")
# SHAPä½¿ç”¨ç¤ºä¾‹
model_path = r"E:\Desktop\5Gsimulation_XGboost\pythonProject\xgb_optuna_model_processed_feature_engineered.pkl"
csv_path = r"table/fishnet_30x30_processed_feature_engineered.csv"
# #
shap_selected_features(model_path, csv_path, selected_features=None)
# plot_pdp_selected_features(model_path, csv_path)
